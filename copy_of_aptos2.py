# -*- coding: utf-8 -*-
"""Copy of aptos2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BR7SfHLodTTWYcWniIIt4bl3IuzLK7m-
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing import image_dataset_from_directory
from keras.models import Sequential
from sklearn.metrics import confusion_matrix
from keras.layers import Conv2D,MaxPooling2D,Dropout,Dense,Flatten,BatchNormalization,Rescaling
from keras.datasets import mnist
from keras.losses import sparse_categorical_crossentropy,squared_hinge
from keras.optimizers import Adam,SGD
from keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical,load_img,array_to_img
import numpy as np
from matplotlib import pyplot as plt
from keras import Model
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

import cv2
from PIL import Image

from keras import layers
from tensorflow.keras import applications
from keras.applications import MobileNetV2,VGG19,ResNet50
from keras.callbacks import Callback, ModelCheckpoint
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.optimizers import Adam
import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix,classification_report

from tqdm import tqdm

train_df = pd.read_csv('/content/drive/MyDrive/aptos 2019/train_1.csv')
test_df = pd.read_csv('/content/drive/MyDrive/aptos 2019/test.csv')
valid_df=pd.read_csv('/content/drive/MyDrive/aptos 2019/valid.csv')
print(train_df.shape)
print(test_df.shape)
print(test_df.head())
train_df.columns

y_train = train_df['diagnosis']
y_val = test_df['diagnosis']
y_valid=valid_df['diagnosis']

print(y_train.shape)

print(y_val.shape)
print(y_valid.shape)

x_train_vgg19_features=np.load("/content/drive/MyDrive/aptos 2019/vgg19_X_train.npy")
x_val_vgg19_features=np.load("/content/drive/MyDrive/aptos 2019/vgg19_X_val.npy")
x_train_resnet50_features=np.load("/content/drive/MyDrive/aptos 2019/resnet_X_train.npy")
x_val_resnet50_features=np.load("/content/drive/MyDrive/aptos 2019/resnet_X_val.npy")
x_valid_vgg19_features=np.load("/content/drive/MyDrive/aptos 2019/vgg19_X_valid.npy")
x_valid_resnet50_features=np.load("/content/drive/MyDrive/aptos 2019/resnet_X_valid.npy")

x_valid_vgg19_features=np.load("/content/drive/MyDrive/aptos 2019/vgg19_X_valid.npy")
x_valid_resnet50_features=np.load("/content/drive/MyDrive/aptos 2019/resnet_X_valid.npy")

from tensorflow.keras.layers import Input, Concatenate, Dense
from tensorflow.keras.models import Model
BATCH_SIZE=32
# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Define the neural network layers
dense_1 = Dense(512, activation='relu')(input_vgg19)
dense_2 = Dense(256, activation='relu')(dense_1)
dense_3 = Dense(128, activation='relu')(dense_2)

# Cross-pooling layer to combine VGG19 and ResNet50 features
cross_pooled = Concatenate()([dense_3, input_resnet50])

dense_4 = Dense(64, activation='relu')(cross_pooled)
dense_5 = Dense(32, activation='relu')(dense_4)
output = Dense(5, activation='softmax')(dense_5)

# Create the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    steps_per_epoch=x_train_vgg19_features.shape[0] / BATCH_SIZE,
    epochs=100,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)     #30 epochs

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)    #50 epochs

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)  #50 epochs

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)  #100 epochs

from tensorflow.keras.layers import Input, Concatenate, Dense
from tensorflow.keras.models import Model
BATCH_SIZE=32
# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Define the neural network layers
dense_1 = Dense(512, activation='relu')(input_vgg19)
dense_2 = Dense(256, activation='relu')(dense_1)
dense_3 = Dense(128, activation='relu')(dense_2)

# Cross-pooling layer to combine VGG19 and ResNet50 features
cross_pooled = Concatenate()([dense_3, input_resnet50])

dense_4 = Dense(64, activation='relu')(cross_pooled)
dense_5 = Dense(32, activation='relu')(dense_4)
output = Dense(5, activation='softmax')(dense_5)

# Create the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    #steps_per_epoch=x_train_vgg19_features.shape[0] / BATCH_SIZE,
    epochs=100,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)    #50 epochs

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)    #100 epochs

from tensorflow.keras.layers import Input, Concatenate, Dense
from tensorflow.keras.models import Model
BATCH_SIZE=32
# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Define the neural network layers
dense_1 = Dense(512, activation='relu')(input_vgg19)
dense_2 = Dense(256, activation='relu')(dense_1)
dense_3 = Dense(128, activation='relu')(dense_2)

# Cross-pooling layer to combine VGG19 and ResNet50 features
cross_pooled = Concatenate()([dense_3, input_resnet50])

dense_4 = Dense(64, activation='relu')(cross_pooled)
dense_5 = Dense(32, activation='relu')(dense_4)
output = Dense(5, activation='softmax')(dense_5)

# Create the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    steps_per_epoch=x_train_vgg19_features.shape[0] / BATCH_SIZE,batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)    #50 epochs

from tensorflow.keras.layers import Input, Concatenate, Dense
from tensorflow.keras.models import Model
BATCH_SIZE=32
# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Define the neural network layers
dense_1 = Dense(256, activation='relu')(input_vgg19)
#dense_2 = Dense(256, activation='relu')(dense_1)
#dense_3 = Dense(128, activation='relu')(dense_2)

# Cross-pooling layer to combine VGG19 and ResNet50 features
cross_pooled = Concatenate()([dense_1, input_resnet50])

#dense_4 = Dense(64, activation='relu')(cross_pooled)
dense_5 = Dense(128, activation='relu')(cross_pooled)
output = Dense(5, activation='softmax')(dense_5)

# Create the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    steps_per_epoch=x_train_vgg19_features.shape[0] / BATCH_SIZE,batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)    #50 epochs

from tensorflow.keras.layers import Input, Concatenate, Dense
from tensorflow.keras.models import Model
BATCH_SIZE=32
# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Define the neural network layers
dense_1 = Dense(512, activation='relu')(input_vgg19)
dense_2 = Dense(256, activation='relu')(dense_1)
dense_3 = Dense(256, activation='relu')(dense_2)
dense_03 = Dense(128, activation='relu')(dense_3)
# Cross-pooling layer to combine VGG19 and ResNet50 features
cross_pooled = Concatenate()([dense_1, input_resnet50])

dense_4 = Dense(128, activation='relu')(cross_pooled)
dense_04 = Dense(64, activation='relu')(dense_4)
dense_5 = Dense(32, activation='relu')(dense_04)
output = Dense(5, activation='softmax')(dense_5)

# Create the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    steps_per_epoch=x_train_vgg19_features.shape[0] / BATCH_SIZE,batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)    #50 epochs

from tensorflow.keras.layers import Input, Concatenate, Dense
from tensorflow.keras.models import Model
BATCH_SIZE=16
# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Define the neural network layers
dense_1 = Dense(512, activation='relu')(input_vgg19)
dense_2 = Dense(256, activation='relu')(dense_1)
dense_3 = Dense(256, activation='relu')(dense_2)
dense_03 = Dense(128, activation='relu')(dense_3)
# Cross-pooling layer to combine VGG19 and ResNet50 features
cross_pooled = Concatenate()([dense_1, input_resnet50])

dense_4 = Dense(128, activation='relu')(cross_pooled)
dense_04 = Dense(64, activation='relu')(dense_4)
dense_5 = Dense(32, activation='relu')(dense_04)
output = Dense(5, activation='softmax')(dense_5)

# Create the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    steps_per_epoch=x_train_vgg19_features.shape[0] / BATCH_SIZE,batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)    #50 epochs

from tensorflow.keras.layers import Input, Concatenate, Dense
from tensorflow.keras.models import Model
BATCH_SIZE=16
# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Define the neural network layers
dense_1 = Dense(512, activation='relu')(input_vgg19)
dense_2 = Dense(, activation='relu')(dense_1)
dense_2 = Dense(128, activation='relu')(dense_1)
# Cross-pooling layer to combine VGG19 and ResNet50 features
cross_pooled = Concatenate()([dense_1, input_resnet50])

dense_04 = Dense(64, activation='relu')(cross_pooled)
dense_5 = Dense(32, activation='relu')(dense_04)
output = Dense(5, activation='softmax')(dense_5)

# Create the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    steps_per_epoch=x_train_vgg19_features.shape[0] / BATCH_SIZE,batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)    #50 epochs

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming you have already trained your model and have the validation data

# Predict probabilities for each class using validation data
y_pred_probs = model.predict([x_valid_vgg19_features, x_valid_resnet50_features])

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(5):  # Assuming 5 classes
    fpr[i], tpr[i], _ = roc_curve(y_valid, y_pred_probs[:, i], pos_label=i)
    roc_auc[i] = auc(fpr[i], tpr[i])
l=['Normal','Mild','Moderate','Intense','Proliferative']
# Plot ROC curve for each class
plt.figure(figsize=(8, 6))
for i in range(5):
    plt.plot(fpr[i], tpr[i], label=f'Class {l[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

import matplotlib.pyplot as plt

# Assuming 'history' is the output from the 'fit' method of your model

# Extract accuracy and validation accuracy
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

# Extract the number of epochs
epochs = range(1, len(acc) + 1)

# Plot training and validation accuracy
plt.figure(figsize=(10, 6))
plt.plot(epochs, acc,  label='Training Accuracy')
plt.plot(epochs, val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.legend()

plt.show()

from sklearn.metrics import confusion_matrix
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from tensorflow.keras.utils import to_categorical

# Assuming you already have a model and validation data prepared
# Predict classes

y_pred_classes = np.argmax(y_pred_probs, axis=1)

# If your y_val is in one-hot encoded format, convert it back to class indices
y_true = np.argmax(y_valid, axis=1) if y_valid.ndim > 1 else y_valid

# Compute the confusion matrix
cm = confusion_matrix(y_valid, y_pred_classes)

# Normalize the confusion matrix
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
l=['Normal','Mild','Moderate','Intense','Proliferative']

# Plotting
plt.figure(figsize=(10, 7))
sns.heatmap(cm_normalized, annot=True, fmt=".2%", cmap='Blues', cbar=False,
            xticklabels=[f' {l[i]}' for i in range(cm.shape[1])],
            yticklabels=[f' {l[i]}' for i in range(cm.shape[0])])
plt.title('Normalized Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_pred_classes,y_true))





from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, ActivityRegularization
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import l1

# Load your previously prepared combined features
# Assuming x_train_combined, x_val_combined
x_train_combined = np.hstack((x_train_vgg19_features, x_train_resnet50_features))
x_val_combined = np.hstack((x_val_vgg19_features, x_val_resnet50_features))
# Step 1: Define the Sparse Autoencoder
input_shape = x_train_combined.shape[1]
input_layer = Input(shape=(input_shape,))
# Encoder
encoded = Dense(256, activation='relu', activity_regularizer=l1(1e-5))(input_layer)  # Sparse representation
# Decoder
decoded = Dense(input_shape, activation='sigmoid')(encoded)

# Autoencoder Model
autoencoder = Model(inputs=input_layer, outputs=decoded)
autoencoder.compile(optimizer='adam', loss='mse')

# Step 2: Train Autoencoder
autoencoder.fit(x_train_combined, x_train_combined,
                epochs=50,
                batch_size=256,
                shuffle=True,
                validation_data=(x_val_combined, x_val_combined))

# Step 3: Define Encoder Model for Feature Extraction
encoder = Model(inputs=input_layer, outputs=encoded)

# Extract Encoded Features
x_train_encoded = encoder.predict(x_train_combined)
x_val_encoded = encoder.predict(x_val_combined)

# Step 4: Train a DNN with the Encoded Features
dnn_model = Sequential([
    Dense(512, activation='relu', input_shape=(256,)),  # Adjust input shape based on encoder output
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(5, activation='softmax')
])
dnn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train DNN
history = dnn_model.fit(
    x_train_encoded, y_train,
    epochs=50,
    batch_size=32,
    validation_data=(x_val_encoded, y_val)
)

x_valid_combined = np.hstack((x_valid_vgg19_features, x_valid_resnet50_features))
x_valid_encoded = encoder.predict(x_valid_combined)
dnn_model.evaluate(x_valid_encoded, y_valid)

dnn_model = Sequential([
    Dense(512, activation='relu', input_shape=(256,)),  # 1st hidden layer
    Dense(256, activation='relu'),  # 2nd hidden layer
    Dense(128, activation='relu'),  # 3rd hidden layer
    Dense(64, activation='relu'),   # 4th hidden layer
    Dense(32, activation='relu'),   # 5th hidden layer
    Dense(5, activation='softmax')  # Output layer
])
dnn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train DNN
history = dnn_model.fit(
    x_train_encoded, y_train,
    epochs=50,
    batch_size=32,
    validation_data=(x_val_encoded, y_val)
)

dnn_model.evaluate(x_valid_encoded, y_valid)

from tensorflow.keras.layers import Input, Concatenate, Dense
from tensorflow.keras.models import Model
BATCH_SIZE=16
# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Define the neural network layers
dense_1 = Dense(512, activation='relu')(input_vgg19)
dense_2 = Dense(256, activation='relu')(dense_1)
dense_3 = Dense(256, activation='relu')(dense_2)
dense_03 = Dense(128, activation='relu')(dense_3)
# Cross-pooling layer to combine VGG19 and ResNet50 features
cross_pooled = Concatenate()([dense_1, input_resnet50])

dense_4 = Dense(128, activation='relu')(cross_pooled)
dense_04 = Dense(64, activation='relu')(dense_4)
dense_5 = Dense(32, activation='relu')(dense_04)
output = Dense(5, activation='softmax')(dense_5)

# Create the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    steps_per_epoch=x_train_vgg19_features.shape[0] / BATCH_SIZE,batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)    #50 epochs

model.save('multi_model.keras')

!mkdir -p saved_model
model.save('saved_model/multi_model')

new_model = tf.keras.models.load_model('saved_model/multi_model')
new_model.summary()

model.save('multi_model.h5')

from tensorflow.keras.layers import Input, Concatenate, Dense, Dropout, BatchNormalization
from tensorflow.keras.models import Model
BATCH_SIZE = 16

# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Define the neural network layers for VGG19 pathway
dense_1 = Dense(512, activation='relu')(input_vgg19)
bn1 = BatchNormalization()(dense_1)
dropout1 = Dropout(0.25)(bn1)  # Dropout rate of 50%

dense_2 = Dense(256, activation='relu')(dropout1)

dropout2 = Dropout(0.25)(dense_2)  # Consistent dropout rate


# Concatenate VGG19 processed features with ResNet50 features
cross_pooled = Concatenate()([dropout2, input_resnet50])

# Further processing after concatenation
dense_4 = Dense(128, activation='relu')(cross_pooled)
bn4 = BatchNormalization()(dense_4)
dropout4 = Dropout(0.25)(bn4)


# Final output layer
output = Dense(5, activation='softmax')(dropout4)

# Create and compile the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    steps_per_epoch=x_train_vgg19_features.shape[0] // BATCH_SIZE,
    batch_size=BATCH_SIZE,
    epochs=100,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)    #50 epochs  remving 1 layer from 4 layers

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)    #50 epochs removing 1bn of 3 layers

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)    #50 epochs removing 2bn of 3 layers

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)    #50 epochs removing 1bn,1dp of 3 layers

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)    #100 epochs removing 1bn of 3 layers

from tensorflow.keras.layers import Input, Concatenate, Dense, Dropout, BatchNormalization
from tensorflow.keras.models import Model
BATCH_SIZE = 16

# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Define the neural network layers for VGG19 pathway
dense_1 = Dense(512, activation='relu')(input_vgg19)
bn1 = BatchNormalization()(dense_1)
dropout1 = Dropout(0.25)(bn1)  # Dropout rate of 50%

dense_2 = Dense(256, activation='relu')(dropout1)
bn2 = BatchNormalization()(dense_2)
dropout2 = Dropout(0.25)(bn2)  # Consistent dropout rate

dense_3 = Dense(128, activation='relu')(dropout2)
bn3 = BatchNormalization()(dense_3)
dropout3 = Dropout(0.25)(bn3)

# Concatenate VGG19 processed features with ResNet50 features
cross_pooled = Concatenate()([dropout3, input_resnet50])

# Further processing after concatenation
dense_4 = Dense(64, activation='relu')(cross_pooled)
bn4 = BatchNormalization()(dense_4)
dropout4 = Dropout(0.25)(bn4)

dense_5 = Dense(32, activation='relu')(dropout4)
bn5 = BatchNormalization()(dense_5)
dropout5 = Dropout(0.25)(bn5)

# Final output layer
output = Dense(5, activation='softmax')(dropout5)

# Create and compile the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    steps_per_epoch=x_train_vgg19_features.shape[0] // BATCH_SIZE,
    batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid) # 5layers, 0.5dp

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid) # 5layers, 0.25dp

from tensorflow.keras.layers import Input, Concatenate, Dense, Dropout, BatchNormalization
from tensorflow.keras.models import Model
BATCH_SIZE = 16

# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Define the neural network layers for VGG19 pathway
dense_1 = Dense(512, activation='relu')(input_vgg19)
bn1 = BatchNormalization()(dense_1)

dense_2 = Dense(256, activation='relu')(bn1)
bn2 = BatchNormalization()(dense_2)

dense_3 = Dense(128, activation='relu')(bn2)
bn3 = BatchNormalization()(dense_3)

# Concatenate VGG19 processed features with ResNet50 features
cross_pooled = Concatenate()([bn3, input_resnet50])

# Further processing after concatenation
dense_4 = Dense(64, activation='relu')(cross_pooled)
bn4 = BatchNormalization()(dense_4)

dense_5 = Dense(32, activation='relu')(bn4)
bn5 = BatchNormalization()(dense_5)

# Final output layer
output = Dense(5, activation='softmax')(bn5)

# Create and compile the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    steps_per_epoch=x_train_vgg19_features.shape[0] // BATCH_SIZE,
    batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid) # no-dp,5bn

from tensorflow.keras.layers import Input, Concatenate, Dense, Dropout, BatchNormalization
from tensorflow.keras.models import Model
BATCH_SIZE = 16

# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Define the neural network layers for VGG19 pathway
dense_1 = Dense(512, activation='relu')(input_vgg19)
bn1 = BatchNormalization()(dense_1)

dense_2 = Dense(256, activation='relu')(bn1)
bn2 = BatchNormalization()(dense_2)



# Concatenate VGG19 processed features with ResNet50 features
cross_pooled = Concatenate()([bn2, input_resnet50])

# Further processing after concatenation
dense_4 = Dense(128, activation='relu')(cross_pooled)
bn4 = BatchNormalization()(dense_4)


# Final output layer
output = Dense(5, activation='softmax')(bn4)

# Create and compile the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    steps_per_epoch=x_train_vgg19_features.shape[0] // BATCH_SIZE,
    batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)   # 3 layers,no-dp,3 bn

from tensorflow.keras.layers import Input, Concatenate, Dense, Dropout, BatchNormalization
from tensorflow.keras.models import Model
BATCH_SIZE = 16

# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Define the neural network layers for VGG19 pathway
dense_1 = Dense(512, activation='relu')(input_vgg19)

dense_2 = Dense(256, activation='relu')(dense_1)



# Concatenate VGG19 processed features with ResNet50 features
cross_pooled = Concatenate()([dense_2, input_resnet50])

# Further processing after concatenation
dense_4 = Dense(128, activation='relu')(cross_pooled)


# Final output layer
output = Dense(5, activation='softmax')(dense_4)

# Create and compile the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    steps_per_epoch=x_train_vgg19_features.shape[0] // BATCH_SIZE,
    batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)



x_train_vgg19_features=np.load("/content/drive/MyDrive/aptos 2019/vgg19_X_train.npy")
x_val_vgg19_features=np.load("/content/drive/MyDrive/aptos 2019/vgg19_X_val.npy")

x_train=np.load("/content/drive/MyDrive/aptos 2019/X_train1.npy")
x_test=np.load("/content/drive/MyDrive/aptos 2019/X_val.npy")
x_valid=np.load('/content/drive/MyDrive/aptos 2019/X_valid.npy')

from sklearn.preprocessing import StandardScaler
from keras import backend as K
from keras.layers import Input, Dense, Lambda
x_train_flattened = x_train.reshape((x_train.shape[0], -1))
x_test_flattened = x_test.reshape((x_test.shape[0], -1))
x_valid_flattened = x_test.reshape((x_valid.shape[0], -1))
# Define and train an undercomplete autoencoder
encoder = Sequential([
    Dense(128, activation='relu', input_shape=(x_train_flattened.shape[1],)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu')  # Set bottleneck layer size to 32
])

decoder = Sequential([
    Dense(64, activation='relu', input_shape=(32,)),  # Reverse the bottleneck layer size
    Dense(128, activation='relu'),
    Dense(x_train_flattened.shape[1], activation='relu')
])

autoencoder = Sequential([encoder, decoder])
autoencoder.compile(optimizer='adam', loss='mean_squared_error')
autoencoder.fit(x_train_flattened, x_train_flattened, epochs=30, batch_size=32, validation_split=0.2)

# Extract features using the trained autoencoder
encoded_train_features = encoder.predict(x_train_flattened)
encoded_test_features = encoder.predict(x_test_flattened)
encoded_valid_features = encoder.predict(x_valid_flattened)

from tensorflow.keras.applications import DenseNet121, MobileNetV2
import numpy as np

# Load DenseNet121 and MobileNetV2 with pre-trained ImageNet weights
densenet = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
mobilenet = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

def extract_features(model, x_data):
    features = model.predict(x_data, batch_size=32, verbose=1)
    features_flattened = features.reshape((features.shape[0], -1))
    return features_flattened

# Assuming 'x_valid' is your validation dataset pre-processed to the required input size (224x224)
x_train_densenet_features = extract_features(densenet, x_train)
x_train_mobilenet_features = extract_features(mobilenet, x_train)
x_test_densenet_features = extract_features(densenet, x_test)
x_test_mobilenet_features = extract_features(mobilenet, x_valid)
x_valid_densenet_features = extract_features(densenet, x_valid)
x_valid_mobilenet_features = extract_features(mobilenet, x_valid)
# Combine the features from both models
x_valid_combined = np.hstack((x_valid_densenet_features, x_valid_mobilenet_features))

import numpy as np
np.save("/content/drive/MyDrive/aptos 2019/x_train_densenet_features.npy",x_train_densenet_features)
np.save("/content/drive/MyDrive/aptos 2019/x_train_mobilenet_features.npy",x_train_mobilenet_features)
np.save("/content/drive/MyDrive/aptos 2019/x_test_densenet_features.npy",x_test_densenet_features)
np.save("/content/drive/MyDrive/aptos 2019/x_test_mobilenet_features.npy",x_test_mobilenet_features)
np.save("/content/drive/MyDrive/aptos 2019/x_valid_densenet_features.npy",x_valid_densenet_features)
np.save("/content/drive/MyDrive/aptos 2019/x_valid_mobilenet_features.npy",x_valid_mobilenet_features)

x_train_densenet_features=np.load("/content/drive/MyDrive/aptos 2019/x_train_densenet_features.npy")
x_train_mobilenet_features=np.load("/content/drive/MyDrive/aptos 2019/x_train_mobilenet_features.npy")
x_test_densenet_features=np.load("/content/drive/MyDrive/aptos 2019/x_test_densenet_features.npy")
x_test_mobilenet_features=np.load("/content/drive/MyDrive/aptos 2019/x_test_mobilenet_features.npy")
x_valid_densenet_features=np.load("/content/drive/MyDrive/aptos 2019/x_valid_densenet_features.npy")
x_valid_mobilenet_features=np.load("/content/drive/MyDrive/aptos 2019/x_valid_mobilenet_features.npy")

from tensorflow.keras.layers import Input, Concatenate, Dense
from tensorflow.keras.models import Model
BATCH_SIZE=16
# Define input layers for VGG19 and ResNet50 features
input_densenet = Input(shape=(x_train_densenet_features.shape[1],))
input_mobilenet = Input(shape=(x_train_mobilenet_features.shape[1],))

# Define the neural network layers
dense_1 = Dense(512, activation='relu')(input_densenet)
dense_2 = Dense(256, activation='relu')(dense_1)
dense_3 = Dense(256, activation='relu')(dense_2)
dense_03 = Dense(128, activation='relu')(dense_3)
# Cross-pooling layer to combine VGG19 and ResNet50 features
cross_pooled = Concatenate()([dense_1, input_mobilenet])

dense_4 = Dense(128, activation='relu')(cross_pooled)
dense_04 = Dense(64, activation='relu')(dense_4)
dense_5 = Dense(32, activation='relu')(dense_04)
output = Dense(5, activation='softmax')(dense_5)

# Create the model
model = Model(inputs=[input_densenet, input_mobilenet], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_densenet_features, x_train_mobilenet_features], y_train,
    steps_per_epoch=x_train_densenet_features.shape[0] / BATCH_SIZE,batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=([x_test_densenet_features, x_test_mobilenet_features], y_val)
)

model.evaluate([x_valid_densenet_features, x_valid_mobilenet_features], y_valid)

import pandas as pd
import seaborn as sns
sns.set()
# Create a list of names and scores
names = [ 'InceptionV3 + MLPÂ [10]', 'MobileNet + DNN [10]',
         'VGG & Xception + DNN [5]', 'Composite Gated Attention DNN [3]','Modified xception [10]','VGG19 & RESNET50 + DNN(proposed)']
scores = [78.72, 79.01, 80.96, 82.54,83.09,83.33]

# Create a DataFrame using a dictionary
df = pd.DataFrame({
    'models': names,
    'accuracy': scores
})

# Print the DataFrame

import pandas as pd
import matplotlib.pyplot as plt



# Plotting
plt.figure(figsize=(10, 6))  # Set the figure size (optional)
bars=plt.bar(df['models'], df['accuracy'], color='skyblue',label='accuracy')  # Create a bar plot
for bar, value in zip(bars, df['accuracy']):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, f'{value:.2f}', ha='center', va='bottom')
plt.xlabel('Model')  # Label on X-axis
plt.ylabel('Accuracy')  # Label on Y-axis
plt.plot(df['models'], df['accuracy'], marker='o', color='black', label='Accuracy Trend')
plt.xticks(rotation=60)
plt.xticks(fontsize=10)
plt.legend()  # Show the legend
plt.ylim(70, 90)  # Limit for Y-axis (optional)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Create a DataFrame
names = ['NASNet + t-SNE + SVM [18]', 'InceptionV3 + MLP [19]', 'MobileNet [19]',
         'Blended VGG & Xception + DNN [20]', 'Composite Gated Attention DNN [21]',
         'Modified Xception [19]', 'VGG19 & RESNET50 + DNN (proposed)']
scores = [77.90, 78.72, 79.01, 80.96, 82.54, 83.09, 83.33]
df = pd.DataFrame({'models': names, 'accuracy': scores})

# Plotting
plt.figure(figsize=(10, 6))  # Set the figure size (optional)

# Bar plot
plt.bar(df['models'], df['accuracy'], color='skyblue', label='Accuracy', width=0.5)

# Line plot
plt.plot(df['models'], df['accuracy'], marker='o', color='red', label='Accuracy Trend')

# Customize the plot

plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.xticks(rotation=90, fontsize=10)
plt.title('Comparison of Proposed Models with Existing Models')
plt.ylim(70, 90)  # Limit for Y-axis (optional)
plt.legend()  # Show the legend
plt.grid(axis='y')  # Show grid lines on the y-axis
plt.tight_layout()  # Adjust layout to prevent clipping of labels

plt.show()

from tensorflow.keras.layers import Input, Concatenate, Dense
from tensorflow.keras.models import Model
BATCH_SIZE=16
# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Define the neural network layers
dense_1 = Dense(512, activation='relu')(input_vgg19)
cross_pooled = Concatenate()([dense_1, input_resnet50])
dense_2 = Dense(128, activation='relu')(cross_pooled)
dense_3 = Dense(64, activation='relu')(dense_2)
dense_4 = Dense(32, activation='relu')(dense_3)
output = Dense(5, activation='softmax')(dense_4)

# Create the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    steps_per_epoch=x_train_vgg19_features.shape[0] / BATCH_SIZE,batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

(model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)

from tensorflow.keras.layers import Input, Concatenate, Dense
from tensorflow.keras.models import Model
import numpy as np
BATCH_SIZE=16
# Define input layers for VGG19 and ResNet50 features

x_train_combined = np.hstack((x_train_vgg19_features, x_train_resnet50_features))# Define the neural network layers
x_test_combined = np.hstack((x_val_vgg19_features, x_val_resnet50_features))
input_combine = Input(shape=(x_train_combined.shape[1],))
dense_1 = Dense(512, activation='relu')(input_combine)
dense_2 = Dense(128, activation='relu')(dense_1)
dense_3 = Dense(64, activation='relu')(dense_2)
dense_4 = Dense(32, activation='relu')(dense_3)
output = Dense(5, activation='softmax')(dense_4)

# Create the model
model = Model(inputs=[input_combine], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    x_train_combined, y_train,
    steps_per_epoch=x_train_combined.shape[0] / BATCH_SIZE,batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=(x_test_combined, y_val)
)

(x_valid_combined = np.hstack((x_valid_vgg19_features, x_valid_resnet50_features))
model.evaluate(x_valid_combined,y_valid)

from tensorflow.keras.layers import Input, Concatenate, Dense
from tensorflow.keras.models import Model

BATCH_SIZE = 16

# Assuming x_train_vgg19_features and x_train_resnet50_features are your training datasets from VGG19 and ResNet50
# Similarly for x_val_vgg19_features and x_val_resnet50_features for validation datasets
# These should be loaded or defined earlier in your code

# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Concatenate features
concatenated_features = Concatenate()([input_vgg19, input_resnet50])

# Define the neural network layers
dense_1 = Dense(512, activation='relu')(concatenated_features)
dense_2 = Dense(128, activation='relu')(dense_1)
dense_3 = Dense(64, activation='relu')(dense_2)
dense_4 = Dense(32, activation='relu')(dense_3)
output = Dense(5, activation='softmax')(dense_4)

# Create the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    steps_per_epoch=len(x_train_vgg19_features) / BATCH_SIZE,
    batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)

from tensorflow.keras.layers import Input, Concatenate, Dense
from tensorflow.keras.models import Model
import numpy as np
BATCH_SIZE=16
# Define input layers for VGG19 and ResNet50 features

x_train_combined = np.concatenate((x_train_vgg19_features, x_train_resnet50_features),axis=1)# Define the neural network layers
x_test_combined = np.concatenate((x_val_vgg19_features, x_val_resnet50_features),axis=1)
input_combine = Input(shape=(x_train_combined.shape[1],))
dense_1 = Dense(512, activation='relu')(input_combine)
dense_2 = Dense(128, activation='relu')(dense_1)
dense_3 = Dense(64, activation='relu')(dense_2)
dense_4 = Dense(32, activation='relu')(dense_3)
output = Dense(5, activation='softmax')(dense_4)

# Create the model
model = Model(inputs=[input_combine], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    x_train_combined, y_train,
    steps_per_epoch=x_train_combined.shape[0] / BATCH_SIZE,batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=(x_test_combined, y_val)
)

x_valid_combined = np.concatenate((x_valid_vgg19_features, x_valid_resnet50_features),axis=1)
model.evaluate(x_valid_combined,y_valid)

x_train=np.load("/content/drive/MyDrive/aptos 2019/X_train1.npy")
x_val=np.load("/content/drive/MyDrive/aptos 2019/X_val.npy")
x_valid=np.load("/content/drive/MyDrive/aptos 2019/X_valid.npy")

import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import VGG19, ResNet50
from tensorflow.keras.layers import Input, GlobalAveragePooling2D, GlobalMaxPooling2D, Concatenate, Dense
from tensorflow.keras.models import Model

vgg19 = VGG19(weights='imagenet', include_top=False,input_shape=(224,224,3))
resnet50 = ResNet50(weights='imagenet', include_top=False,input_shape=(224,224,3))

vgg19_output = vgg19.output
resnet50_output = resnet50.output
# Apply Global Average Pooling and Global Max Pooling to each model's output
vgg19_avg_pool = GlobalAveragePooling2D()(vgg19_output)
resnet50_avg_pool = GlobalAveragePooling2D()(resnet50_output)
vgg19_max_pool = GlobalMaxPooling2D()(vgg19_output)
resnet50_max_pool = GlobalMaxPooling2D()(resnet50_output)

# Cross-Model Pooling - Average and Max
average_pooled = Concatenate()([vgg19_avg_pool, resnet50_avg_pool])
max_pooled = Concatenate()([vgg19_max_pool, resnet50_max_pool])

# Concatenate the average and max pooling outputs
combined_features = Concatenate()([average_pooled, max_pooled])
input_combine = Input(shape=(combined_features.shape[1],))
dense_1 = Dense(512, activation='relu')(combined_features)
dense_2 = Dense(128, activation='relu')(dense_1)
dense_3 = Dense(64, activation='relu')(dense_2)
dense_4 = Dense(32, activation='relu')(dense_3)
output = Dense(10, activation='softmax')(dense_4)  # Change the number of classes if necessary
# Create the model
final_model = Model(inputs=[vgg19.input, resnet50.input], outputs=output)
final_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Training is performed assuming you have the feature arrays prepared similar to x_train_combined
# Note: Ensure that x_train and y_train are formatted correctly and correspond to the extracted features
# and prepared labels.

BATCH_SIZE=32
history = final_model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    steps_per_epoch=len(x_train_vgg19_features) / BATCH_SIZE,
    batch_size=BATCH_SIZE,
    epochs=50,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

import numpy as np
from tensorflow.keras.applications import VGG19, ResNet50
from tensorflow.keras.layers import Input, GlobalAveragePooling2D, GlobalMaxPooling2D, Concatenate, Dense
from tensorflow.keras.models import Model

# Assuming x_train_vgg and x_train_resnet are your extracted features from VGG19 and ResNet50
# Shape of x_train_vgg: (num_samples, vgg_features)
# Shape of x_train_resnet: (num_samples, resnet_features)

# Define input layers for the extracted features
input_vgg = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet = Input(shape=(x_train_resnet50_features.shape[1],))

# Apply Global Average Pooling and Global Max Pooling to each model's output
vgg_avg_pool = GlobalAveragePooling2D()(input_vgg)
resnet_avg_pool = GlobalAveragePooling2D()(input_resnet)
vgg_max_pool = GlobalMaxPooling2D()(input_vgg)
resnet_max_pool = GlobalMaxPooling2D()(input_resnet)

# Cross-Model Pooling - Average and Max
average_pooled = Concatenate()([vgg_avg_pool, resnet_avg_pool])
max_pooled = Concatenate()([vgg_max_pool, resnet_max_pool])

# Concatenate the average and max pooling outputs
combined_features = Concatenate()([average_pooled, max_pooled])

# Adding dense layers for classification
dense_1 = Dense(512, activation='relu')(combined_features)
dense_2 = Dense(128, activation='relu')(dense_1)
dense_3 = Dense(64, activation='relu')(dense_2)
dense_4 = Dense(32, activation='relu')(dense_3)
output = Dense(10, activation='softmax')(dense_4)  # Assuming 10 classes for classification

# Create the model
model = Model(inputs=[input_vgg, input_resnet], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model (assuming you have corresponding y_train for labels)
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    batch_size=16,
    epochs=50,
   validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

import numpy as np
from tensorflow.keras.applications import VGG19, ResNet50
from tensorflow.keras.layers import Input, GlobalAveragePooling2D, GlobalMaxPooling2D, Concatenate, Dense
from tensorflow.keras.models import Model

# Assuming x_train_vgg19_features and x_train_resnet50_features are your extracted features from VGG19 and ResNet50
# Shape of x_train_vgg19_features: (num_samples, height, width, channels)
# Shape of x_train_resnet50_features: (num_samples, height, width, channels)

# Define input layers for the extracted features
input_vgg = Input(shape=(x_train_vgg19_features.shape[1:]))  # Assuming shape is (height, width, channels)
input_resnet = Input(shape=(x_train_resnet50_features.shape[1:]))

# Apply Global Average Pooling and Global Max Pooling to each model's output
vgg_avg_pool = GlobalAveragePooling2D()(input_vgg)
resnet_avg_pool = GlobalAveragePooling2D()(input_resnet)
vgg_max_pool = GlobalMaxPooling2D()(input_vgg)
resnet_max_pool = GlobalMaxPooling2D()(input_resnet)

# Cross-Model Pooling - Average and Max
average_pooled = Concatenate()([vgg_avg_pool, resnet_avg_pool])
max_pooled = Concatenate()([vgg_max_pool, resnet_max_pool])

# Concatenate the average and max pooling outputs
combined_features = Concatenate()([average_pooled, max_pooled])

# Adding dense layers for classification
dense_1 = Dense(512, activation='relu')(combined_features)
dense_2 = Dense(128, activation='relu')(dense_1)
dense_3 = Dense(64, activation='relu')(dense_2)
dense_4 = Dense(32, activation='relu')(dense_3)
output = Dense(10, activation='softmax')(dense_4)  # Assuming 10 classes for classification

# Create the model
model = Model(inputs=[input_vgg, input_resnet], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model (assuming you have corresponding y_train for labels)
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    batch_size=16,
    epochs=50,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
model = Sequential([
    Dense(512, activation='relu', input_shape=(x_train_vgg19_features.shape[1],)),
    Dropout(0.5),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(5, activation='softmax')  # Assuming there are 10 classes
])
model.compile(optimizer=Adam(),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(x_train_vgg19_features, y_train, epochs=50, batch_size=32, validation_data=(x_val_vgg19_features,y_val))
# To evaluate
test_loss, test_acc = model.evaluate(x_valid_vgg19_features, y_valid)
print('Test accuracy:', test_acc)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
model = Sequential([
    Dense(512, activation='relu', input_shape=(x_train_vgg19_features.shape[1],)),
    Dense(256, activation='relu'),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(5, activation='softmax')  # Assuming there are 10 classes
])
model.compile(optimizer=Adam(),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(x_train_vgg19_features, y_train, epochs=50, batch_size=32, validation_data=(x_val_vgg19_features,y_val))
# To evaluate
test_loss, test_acc = model.evaluate(x_valid_vgg19_features, y_valid)
print('Test accuracy:', test_acc)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
model = Sequential([
    Dense(512, activation='relu', input_shape=(x_train_resnet50_features.shape[1],)),
    Dense(256, activation='relu'),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(5, activation='softmax')  # Assuming there are 10 classes
])
model.compile(optimizer=Adam(),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(x_train_resnet50_features, y_train, epochs=50, batch_size=32, validation_data=(x_val_resnet50_features,y_val))
# To evaluate
test_loss, test_acc = model.evaluate(x_valid_resnet50_features, y_valid)
print('Test accuracy:', test_acc)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np

# x_train_vgg19_features and x_train_resnet50_features must have the same number of samples and feature length
x_train_combined = np.concatenate([x_train_vgg19_features, x_train_resnet50_features],axis=1)
x_val_combined = np.concatenate([x_val_vgg19_features, x_val_resnet50_features],axis=1)

# Assuming both feature sets are the same size and the size is known
input_shape = x_train_combined.shape[1]  # or x_train_resnet50_features.shape[1], as they should be equal

model = Sequential([
    Dense(512, activation='relu', input_shape=(input_shape,)),
    Dense(256, activation='relu'),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(5, activation='softmax')  # Update: Assuming there are 5 classes
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(x_train_combined, y_train, epochs=50, batch_size=32, validation_data=(x_val_combined,y_val))
# Assuming x_test_vgg19_features and x_test_resnet50_features are prepared similarly
x_valid_combined = np.concatenate([x_valid_vgg19_features, x_valid_resnet50_features],axis=1)

test_loss, test_acc = model.evaluate(x_valid_combined, y_valid)
print('Test accuracy:', test_acc)

from tensorflow.keras.layers import Input, Concatenate, Dense
from tensorflow.keras.models import Model
BATCH_SIZE=16
# Define input layers for VGG19 and ResNet50 features
input_vgg19 = Input(shape=(x_train_vgg19_features.shape[1],))
input_resnet50 = Input(shape=(x_train_resnet50_features.shape[1],))

# Define the neural network layers
dense_1 = Dense(512, activation='relu')(input_vgg19)
dense_2 = Dense(256, activation='relu')(dense_1)
dense_3 = Dense(256, activation='relu')(dense_2)
dense_03 = Dense(128, activation='relu')(dense_3)
# Cross-pooling layer to combine VGG19 and ResNet50 features
cross_pooled = Concatenate()([dense_1, input_resnet50])

dense_4 = Dense(128, activation='relu')(cross_pooled)
dense_04 = Dense(64, activation='relu')(dense_4)
dense_5 = Dense(32, activation='relu')(dense_04)
output = Dense(5, activation='softmax')(dense_5)

# Create the model
model = Model(inputs=[input_vgg19, input_resnet50], outputs=output)
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(
    [x_train_vgg19_features, x_train_resnet50_features], y_train,
    epochs=50,
    validation_data=([x_val_vgg19_features, x_val_resnet50_features], y_val)
)

model.evaluate([x_valid_vgg19_features, x_valid_resnet50_features], y_valid)

